import streamlit as st
from openai import OpenAI
import json
import base64
import asyncio
import edge_tts
import os
import tempfile
from streamlit_mic_recorder import mic_recorder

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI å¤šè¯­ç§å£è¯­ç§æ•™", layout="wide", initial_sidebar_state="collapsed")

# --- 2. CSS æ ·å¼ ---
st.markdown("""
    <style>
        .stApp { background-color: #F7F8FA; }
        .main .block-container { padding-bottom: 180px; max-width: 900px; }
        .phase-card {
            background-color: white; border-radius: 12px; padding: 20px;
            margin-bottom: 15px; box-shadow: 0 4px 6px rgba(0,0,0,0.03); border-left: 6px solid #CCC;
        }
        .phase-1 { border-left-color: #4A90E2; }
        .phase-2 { border-left-color: #50C878; background-color: #F0FFF4; }
        .phase-3 { border-left-color: #FF9F43; }
        .phase-header { font-weight: bold; font-size: 1.1rem; margin-bottom: 10px; color: #333; }
        .footer-container {
            position: fixed; bottom: 0; left: 0; right: 0;
            background: rgba(255, 255, 255, 0.98); backdrop-filter: blur(10px);
            padding: 20px 0; border-top: 1px solid #EEE; z-index: 1000;
        }
        audio { height: 35px; width: 100%; margin-top: 10px; }
    </style>
""", unsafe_allow_html=True)

# --- 3. çŠ¶æ€åˆå§‹åŒ– ---
groq_api_key = st.secrets.get("GROQ_API_KEY", "")
if "messages" not in st.session_state: st.session_state.messages = []
if "last_played_id" not in st.session_state: st.session_state.last_played_id = None

# --- 4. ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.title("ğŸ’¡ æ•™ç»ƒè®¾ç½®")
    if not groq_api_key:
        groq_api_key = st.text_input("è¯·è¾“å…¥ Groq API Key", type="password")
    
    # æ–°å¢ï¼šç›®æ ‡è¯­è¨€é€‰æ‹©
    target_lang = st.selectbox("ç›®æ ‡å­¦ä¹ è¯­è¨€", ["è‹±è¯­ (English)", "æ—¥è¯­ (æ—¥æœ¬èª)", "éŸ©è¯­ (í•œêµ­ì–´)", "å¾·è¯­ (Deutsch)", "æ³•è¯­ (FranÃ§ais)"])
    lang_code = target_lang.split(" (")[0]
    
    # åŠ¨æ€åŒ¹é… TTS å£°éŸ³
    voice_options = {
        "è‹±è¯­": {"Ava (ç¾)": "en-US-AvaMultilingualNeural", "Andrew (ç¾)": "en-US-AndrewMultilingualNeural", "Sonia (è‹±)": "en-GB-SoniaNeural"},
        "æ—¥è¯­": {"Nanami": "ja-JP-NanamiNeural", "Keita": "ja-JP-KeitaNeural"},
        "éŸ©è¯­": {"Sun-Hi": "ko-KR-SunHiNeural", "In-Joon": "ko-KR-InJoonNeural"},
        "å¾·è¯­": {"Katja": "de-DE-KatjaNeural", "Killian": "de-DE-KillianNeural"},
        "æ³•è¯­": {"Denise": "fr-FR-DeniseNeural", "Eloise": "fr-FR-EloiseNeural"}
    }
    current_voices = voice_options.get(lang_code, voice_options["è‹±è¯­"])
    voice_name = st.selectbox("æ•™ç»ƒå£°éŸ³", list(current_voices.keys()))
    selected_voice = current_voices[voice_name]

    input_mode = st.radio("å½•å…¥æ¨¡å¼", ["è¯­éŸ³", "æ–‡å­—"])
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²"):
        st.session_state.messages = []
        st.session_state.last_played_id = None
        st.rerun()

groq_client = OpenAI(base_url="https://api.groq.com/openai/v1", api_key=groq_api_key) if groq_api_key else None

# --- 5. æ ¸å¿ƒè¾…åŠ©åŠŸèƒ½ ---
async def get_voice_audio(text, voice):
    if not text or len(text.strip()) == 0: return ""
    try:
        communicate = edge_tts.Communicate(text, voice)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            tmp_path = tmp.name
        await communicate.save(tmp_path)
        with open(tmp_path, "rb") as f: data = f.read()
        os.remove(tmp_path)
        return base64.b64encode(data).decode()
    except: return ""

def get_ai_response(user_text, target_language):
    system_prompt = f"""
    ä½ ç°åœ¨æ˜¯ä¸€åç²¾é€šä¸­è‹±åŒè¯­çš„ä¸“ä¸šå£è¯­æ•™ç»ƒï¼Œç›®å‰æ­£åœ¨æ•™ç”¨æˆ·å­¦ä¹ ã€{target_language}ã€‘ã€‚
    è¯·ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œå¹¶è¾“å‡º JSONï¼š
    1. ã€èº«ä»½ï¼šä¸“ä¸šå¯¼å¸ˆã€‘
       - phase1_correction: é’ˆå¯¹ç”¨æˆ·çš„ã€{target_language}ã€‘æ–‡æœ¬è¿›è¡Œçº é”™å’Œå‘éŸ³/è¯­è°ƒæŒ‡å¯¼ï¼ˆå§‹ç»ˆç”¨ä¸­æ–‡å›ç­”ï¼‰ã€‚
       - phase2_optimized_text: æä¾›ä¸€ä¸ªæœ€åœ°é“ã€å®Œæ•´çš„ä¼˜åŒ–ä¾‹å¥ï¼ˆå¿…é¡»ä»…ä½¿ç”¨ã€{target_language}ã€‘ï¼‰ã€‚
    2. ã€èº«ä»½ï¼šçŸ¥å¿ƒæœ‹å‹ã€‘
       - phase3_interaction: è¯·ç”¨æ‰€åœ¨è¯­è¨€çš„å›½å®¶å±…æ°‘çš„æ­£å¸¸çŠ¶æ€ï¼ˆè¯¥å†…æ•›å°±å†…æ•›ï¼Œè¯¥çƒ­æƒ…å°±çƒ­æƒ…ï¼‰ï¼Œå¯¹ç”¨æˆ·å†…å®¹ç»™äºˆçœŸè¯šçš„å›åº”ï¼Œåˆ†äº«çœ‹æ³•ï¼Œæœ€åè¿½é—®ï¼ˆå¿…é¡»å§‹ç»ˆä½¿ç”¨ã€{target_language}ã€‘ï¼‰ã€‚
    3. phase4_expansion: æä¾› 2 å¥é’ˆå¯¹é˜¶æ®µ 3 çš„åº”ç­”å‚è€ƒï¼ˆå¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œä¸”ä»…ä½¿ç”¨ã€{target_language}ã€‘ï¼‰ã€‚
    
    æ³¨æ„ï¼šé™¤äº† phase1 ç”¨ä¸­æ–‡å¤–ï¼Œå…¶ä½™æ‰€æœ‰æ•™å­¦å’Œäº’åŠ¨å†…å®¹å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ã€{target_language}ã€‘ï¼Œä¸¥ç¦åˆ‡æ¢è¯­è¨€ã€‚
    """
    try:
        response = groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_text}],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        return json.loads(response.choices[0].message.content)
    except: return None

# --- 6. èŠå¤©åŒºæ¸²æŸ“ ---
st.title(f"ğŸ™ï¸ AI {lang_code}å£è¯­ç§æ•™")

if not groq_api_key:
    st.warning("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§é…ç½® API Key")
else:
    for i, msg in enumerate(st.session_state.messages):
        if msg["role"] == "user":
            with st.chat_message("user", avatar="ğŸ‘¤"): st.write(msg["content"])
        else:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                data = msg["content"]
                p1 = data.get("phase1_correction") or data.get("correction") or "AI æš‚æ— ç‚¹è¯„"
                st.markdown(f'<div class="phase-card phase-1"><div class="phase-header">ğŸ”µ AI çº é”™ç‚¹è¯„ (ä¸­æ–‡)</div>{p1}</div>', unsafe_allow_html=True)
                
                p2 = data.get("phase2_optimized_text") or ""
                if p2:
                    st.markdown(f'<div class="phase-card phase-2"><div class="phase-header">ğŸŸ¢ ä¼˜åŒ–è¡¨è¾¾ (ç‚¹å‡»è·Ÿè¯»)</div><span style="font-size:1.2rem; color:#1B5E20;"><b>{p2}</b></span>', unsafe_allow_html=True)
                    opt_audio = asyncio.run(get_voice_audio(p2, selected_voice))
                    if opt_audio: st.markdown(f'<audio src="data:audio/mp3;base64,{opt_audio}" controls></audio></div>', unsafe_allow_html=True)
                    else: st.markdown('</div>', unsafe_allow_html=True)

                p3 = data.get("phase3_interaction") or ""
                st.markdown(f'<div class="phase-card phase-3"><div class="phase-header">ğŸ’¬ äº’åŠ¨äº¤æµ</div>{p3}', unsafe_allow_html=True)
                inter_audio = asyncio.run(get_voice_audio(p3, selected_voice))
                if inter_audio:
                    curr_id = hash(p3)
                    is_new = (i == len(st.session_state.messages) - 1) and (st.session_state.last_played_id != curr_id)
                    autoplay = "autoplay" if is_new else ""
                    st.markdown(f'<audio src="data:audio/mp3;base64,{inter_audio}" {autoplay} controls></audio></div>', unsafe_allow_html=True)
                    if is_new: st.session_state.last_played_id = curr_id
                else: st.markdown('</div>', unsafe_allow_html=True)

                p4 = data.get("phase4_expansion", [])
                if isinstance(p4, list) and len(p4) > 0:
                    tips = " | ".join([f"{idx+1}ï¸âƒ£ {text}" for idx, text in enumerate(p4)])
                    st.markdown(f"<div style='padding-left:15px; margin-bottom:15px;'><small style='color:#888;'>ğŸ’¡ å›åº”å‚è€ƒ: {tips}</small></div>", unsafe_allow_html=True)

# --- 7. åº•éƒ¨è¾“å…¥ä¸æ ¡éªŒ ---
st.markdown('<div class="footer-container">', unsafe_allow_html=True)
cols = st.columns([1, 6, 1])
with cols[1]:
    if input_mode == "è¯­éŸ³":
        audio_in = mic_recorder(start_prompt="ğŸ¤ é•¿æŒ‰å½•éŸ³", stop_prompt="âœ… æ¾å¼€å‘é€", key='recorder', use_container_width=True)
        if audio_in:
            # æ ¡éªŒ1ï¼šæ£€æŸ¥å­—èŠ‚å¤§å°ï¼ˆä¾‹å¦‚å°äº 1000 å­—èŠ‚é€šå¸¸æ˜¯è¯¯è§¦ï¼‰
            if len(audio_in['bytes']) < 1500:
                st.warning("âš ï¸ å½•éŸ³æ—¶é—´è¿‡çŸ­ï¼Œè¯·é•¿æŒ‰å½•åˆ¶å®Œæ•´çš„å¥å­ã€‚")
            else:
                curr_hash = hash(audio_in['bytes'])
                if "last_audio_hash" not in st.session_state or st.session_state.last_audio_hash != curr_hash:
                    st.session_state.last_audio_hash = curr_hash
                    with st.spinner("æ­£åœ¨è¯†åˆ«è¯­éŸ³..."):
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                            tmp.write(audio_in['bytes'])
                            t_path = tmp.name
                        with open(t_path, "rb") as f:
                            transcript = groq_client.audio.transcriptions.create(model="whisper-large-v3", file=f)
                        os.remove(t_path)
                        u_text = transcript.text
                        # æ ¡éªŒ2ï¼šæ£€æŸ¥è¯†åˆ«å‡ºçš„æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
                        if not u_text or len(u_text.strip()) < 2:
                            st.warning("âš ï¸ æ— æ³•è¯†åˆ«æ‚¨çš„è¯­éŸ³ï¼Œè¯·é‡è¯•ã€‚")
                        else:
                            ai_data = get_ai_response(u_text, lang_code)
                            if ai_data:
                                st.session_state.messages.append({"role": "user", "content": u_text})
                                st.session_state.messages.append({"role": "assistant", "content": ai_data})
                                st.rerun()
    else:
        txt_in = st.chat_input(f"ç”¨{lang_code}è¾“å…¥å¥å­...")
        if txt_in:
            ai_data = get_ai_response(txt_in, lang_code)
            if ai_data:
                st.session_state.messages.append({"role": "user", "content": txt_in})
                st.session_state.messages.append({"role": "assistant", "content": ai_data})
                st.rerun()
st.markdown('</div>', unsafe_allow_html=True)


